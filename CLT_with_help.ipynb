{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1MoE3X0lwwyNnxK/st7OZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h1>Invoking the Mersenne Twister and the Central Limit Theorem</h1>\n",
        "</div>\n",
        "\n",
        "In this notebook, we will use the built-in random number generator in Python (the Mersenne Twister) to generate realizations from the uniform distribution over the interval from 0 to 1. We will also see the Central Limit Theorem in action!\n",
        "\n",
        "We begin by loading the minimal required libraries/packages for this lab. Please note that there are multiple ways to call up the Mersenne Twister. For example, there is a package called \"random\" that we are not using here. The reason we are not using it is because the function we would use with that package will produce \"draws\" (realizations) from the uniform(0,1) distribution one at a time. In contrast, there is a function in the basic \"numpy\" package can return many values at once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNC_syTPRE6h"
      },
      "outputs": [],
      "source": [
        "# Load required packages\n",
        "import numpy as np                 # basic numerical package\n",
        "import matplotlib.pyplot as plt    # basic plotting package"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now make an array of 100,000 sampled values for the uniform(0,1) distribution. Why so many? The answer is \"Because we can!\" In this course we want to be able to see our algorithms working without being too distracted by *sampling variability*. For example, suppose that we code up a fair coin flipping algorithm and produce a sample\n",
        "\n",
        "HHTTTHTHHH.\n",
        "\n",
        "This sample has 60% \"heads\" and only 40% \"tails\". Are you ready to say that our algorithm is wrong? Indeed, this is a perfectly reasonable sample for 10 fair coin flips. By generating more values, we will get a better idea whether or not our algorithm is working without the distraction of sampling variability. Sometimes we will not have this luxury because an algorithm is slow. This is not one of those times."
      ],
      "metadata": {
        "id": "0BG7u_Wpq4d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 100,000 values from the uniform(0,1) distribution\n",
        "mysample = np.random.uniform(0,1,100000)"
      ],
      "metadata": {
        "id": "iimAx4cXRlS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's set up bins for a histogram. We'll break up the interval [0,1] into bins of width 0.1. We find that this is a good break width to show the structure in many distributions and usually start with this width unless it is \"obvious\" that it will be a bad idea. For example, if a distribution we are sampling/simulating/drawing from only produces values between 0.003 and 0.007, a bin width of 0.1 is way too wide. All sampled values will fall in in one bin and our histogram will consist of a single rectangle! We will not get to see the shape of the distribution at all. Similarly, using small bin widths when the range of values generated is large is probably also not a good idea."
      ],
      "metadata": {
        "id": "6PyPJsSX0veB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and look at breakpoints for histogram bins as 0,0.1,0.2,...,1.0\n",
        "br = np.arange(0,1,0.1)\n",
        "br\n"
      ],
      "metadata": {
        "id": "E2BfTKorYB4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have produced a range of values from 0 to 1 in increments of 0.1. **Do you see a problem here?** We are not capturing the entire *support* of the distribution. **Go back and fix this before proceeding.**\n",
        "\n",
        "We are ready to make a histogram and to superimpose the uniform (0,1) flat line pdf. To acheive the latter, we usually define an entire array of points to smoothly connect with small lines that look like a curve in the end. Here, we will just define a line between two points."
      ],
      "metadata": {
        "id": "-vehXbb22fbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a histogram\n",
        "plt.hist(mysample,bins=br,density=True,color=\"lightsteelblue\",ec=\"black\",lw=0.5)\n",
        "\n",
        "# Superimpose true pdf\n",
        "# The points (x1,y1)=(0,1) and (x2,y2)=(1,1) are defined by first giving\n",
        "# both x-coordinates and then giving both y-coordinates as [x1,x2] and [y1,y2].\n",
        "# In this case it is the same. :(\n",
        "plt.plot([0,1], [1,1],color=\"darkblue\")\n",
        "\n",
        "# Generate the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0v2PgPd7TXNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this histogram \"flat enough\" for you? (Make sure you have fixed the bin breakpoints to include 1.0 at the end.) In the next lesson, we will talk about how much error to expect in the bar heights when making a histogram based on 100,000 draws even if we are generating the sampled values perfectly. You might want to repeat the last several steps of this lab to see a histogram for other samples of size 100,000."
      ],
      "metadata": {
        "id": "ALMvnISV4JQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "<h3>The Central Limit Theorem in Action</h3>\n",
        "\n",
        "The Central Limit Theorem (CLT) says that if we have a random sample (these are independent and identically distributed random variables) from any distribution with mean $\\mu$ and finite variance $\\sigma^{2}$, the sample mean $\\overline{X}$ will have roughly a normal distribution as long as the sample size is large. (There is a formal \"convergence in distribution\" result here that one would learn about in a [course on Mathematical Statistics](https://www.youtube.com/playlist?list=PLLyj1Zd4UWrOk5-wIki_oOxHJnNj0_437).) In most statistics textbooks, \"large\" means a sample of size greater than 30 or 40. The fact of the matter though is that a sample size at which you will see the normal distribution is heavily dependent on the underlying distribution. Let's try n=40 for the uniform distribution.\n",
        "\n",
        "We will create 100,000 samples of size 40 from the uniform(0,1) distribution and look at the 100,000 different realizations of the sample mean\n",
        "$$\\overline{X} = \\frac{1}{40} \\sum_{i=1}^{40} X_{i}$$\n",
        "\n",
        "Let's make an array, initialized with zeros, to hold all of the sample means."
      ],
      "metadata": {
        "id": "o-of6hMC4vph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbars = np.zeros(100000)"
      ],
      "metadata": {
        "id": "pDI-YEURY1y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 100,000 samples. Compute and store the sample means.\n",
        "# This is a loop where i will take the values 0,1,2,...,99999.\n",
        "for i in range(100000):\n",
        "  mysample2 = np.random.uniform(0,1,40)\n",
        "  xbars[i] = np.mean(mysample2)"
      ],
      "metadata": {
        "id": "baOdX78CUL18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a histogram using our already established breakpoints 0,0.1,0.2,...,1.0."
      ],
      "metadata": {
        "id": "FKCewLld9KPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(xbars,bins=br,density=True,color=\"lightyellow\",ec=\"black\",lw=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uNTltu7F9Hbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this look like a normal distribution to you? The resulting sample means are falling in a narrow interval. Maybe we should use smaller bin widths to see some more detail."
      ],
      "metadata": {
        "id": "pjTVatYe9eIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "br2 = np.arange(0,1,0.005)\n",
        "\n",
        "plt.hist(xbars,bins=br2,density=True,color=\"lightyellow\",ec=\"black\",lw=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w7FUxJAiZMkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are wasting space here. Let's zoom in a little by defining the bins over a narrower range. You may have to adjust the endpoints if they don't capture the full range in your sample. (In this case you will get an error!) We will eventually do this \"dynamically\" so that we don't have to eyeball endpoints."
      ],
      "metadata": {
        "id": "B1Wlu1G3-AcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "br3 = np.arange(0.3,0.7,0.005)\n",
        "plt.hist(xbars,bins=br3,density=True,color=\"lightyellow\",ec=\"black\",lw=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0BvKpQOxZYTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's superimpose the appropriate normal density. Note that the mean and variance for the uniform(0,1) distribution are $\\mu=1/2$ and $\\sigma^{2}=1/12$. (Can you show this on paper?)\n",
        "\n",
        "The CLT says that we should be seeing an approximation to the normal distribution shape with $\\mu_{\\overline{X}}=\\mu$ and $\\sigma_{\\overline{X}}^{2}=\\sigma^{2}/n$. The pdf for this distribution is\n",
        "$$\n",
        "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\overline{X}}^{2}}} \\, exp \\left[{-\\frac{1}{2 \\sigma_{\\overline{X}}^{2}}} \\left( x-\\mu_{\\overline{X}}\\right)^{2}\\right].\n",
        "$$"
      ],
      "metadata": {
        "id": "li-zVGX0_U2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the histogram\n",
        "br3 = np.arange(0.3,0.7,0.005)\n",
        "plt.hist(xbars,bins=br3,density=True,color=\"lightyellow\",ec=\"black\",lw=0.5)\n",
        "\n",
        "# Make a finely meshed sequence of points to be connected for the pdf\n",
        "x = np.linspace(0.3,0.7,100)\n",
        "mu = 0.5\n",
        "sigmasq = (1/12)/40\n",
        "f = (1/np.sqrt(2*np.pi*sigmasq))*np.exp((-1/(2*sigmasq))*(x-mu)**2)\n",
        "\n",
        "# Add the curve to the plot\n",
        "plt.plot(x,f)\n",
        "\n",
        "# Show the result!\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z6gWe6mw-VTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you see the normal distribution, as foretold by the Central Limit Theorem, taking shape? You might want to play with the individual sample size of 40 to see if you can get better results!"
      ],
      "metadata": {
        "id": "a_SuqTEZC881"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSlm289MDK_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}